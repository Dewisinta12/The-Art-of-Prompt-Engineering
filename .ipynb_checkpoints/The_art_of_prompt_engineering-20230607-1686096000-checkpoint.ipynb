{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Art of Prompt Engineering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0TGVEN/images/art%20of%20prompt%20eng.jpeg\" width=\"500\" alt=\"Skills Network Logo\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project delves into the concept of prompt engineering, a crucial aspect of AI that guides AI models to produce desired outputs. It's instrumental in AI tools like chatbots, text summarizers, and content generators, ensuring they communicate accurately, relevantly, and creatively. The project underscores the benefits of prompt engineering in business, from enhancing chatbot performance to guiding personalization systems, enabling accurate sentiment analysis, and optimizing productivity. It emphasizes the importance of clear instructions, examples, keywords, and feedback when working with large language models (LLMs) like ChatGPT. The project also outlines various techniques, including writing clear instructions, giving the model time to \"think\", and strategies like Chain-of-Thought (CoT) Prompting and Generated Knowledge Prompting. We also introduce Langchain library that can provide strong tools for prompt engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importance of Prompt Engineering in AI Tools](#importance-in-ai-tools)\n",
    "3. [Benefits of Prompt Engineering in Business](#benefits-in-business)\n",
    "4. [Prompt Engineering with LLM models like ChatGPT](#prompt-engineering-with-llm)\n",
    "5. [Prompt Engineering Techniques](#prompt-engineering-techniques)\n",
    "    1. [Principle 1: Write Clear and Specific Instructions](#principle-1)\n",
    "    2. [Principle 2: Give the Model Time to “Think”](#principle-2)\n",
    "    3. [Additional Strategies](#additional-strategies)\n",
    "6. [Python Project: Survival Analysis](#python-project)\n",
    "    1. [Understanding the Concept](#understanding-the-concept)\n",
    "    2. [Learning the Basics](#learning-the-basics)\n",
    "    3. [Python Implementation](#python-implementation)\n",
    "    4. [Asking Further Questions](#asking-further-questions)\n",
    "7. [Prompt Engineering with Langchain](#python-project)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "1. Learn to communicate effectively with AI models like ChatGPT.\n",
    "2. Gain hands-on experience in prompt engineering in implementing survival analysis in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Prompt engineering is key in AI, especially in natural language processing. It guides AI models to produce desired outputs like answering questions or generating summaries.\n",
    "\n",
    "## Importance in AI Tools\n",
    "Prompt engineering is vital in AI tools like chatbots, text summarizers, and content generators. It ensures clear communication with the model for accurate, relevant, and creative outputs. For example, in the case of chatbots, it is essential to have suitable inquiries and replies that keep conversations natural and captivating. Likewise, when it comes to text summarizers, they need to present key points and adhere to specified summary length, whereas content generators rely on prompts regarding topic, genre, and tone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Prompt Engineering in Business:\n",
    "\n",
    "Prompt engineering can have a positive impact on productivity and bring various benefits to businesses. Several studies have highlighted the advantages of prompt engineering in improving business outcomes:\n",
    "\n",
    "| Area | Importance of Prompt Engineering | Statistics | Source |\n",
    "| --- | --- | --- | --- |\n",
    "| Chatbot Performance | Ensures accurate and relevant responses, enhancing customer satisfaction and reducing the need for human intervention. | Chatbots will save businesses an estimated $11 billion annually by 2023. | Juniper Research. “Chatbots: Retail, eCommerce, Banking & Healthcare 2018–2023.” |\n",
    "| Content Generation | Boosts the efficiency and quality of content generation processes. | Businesses that prioritize content marketing experience six times higher conversion rates. | HubSpot. “The Ultimate List of Marketing Statistics for 2021.” |\n",
    "| Personalization and Recommendation Systems | Guides systems to understand user preferences and provide relevant suggestions. | Personalization can deliver 40% more revenue and lift revenue by 10 to 15 percent or more. | McKinsey & Company. “The CEO guide to personalization.” |\n",
    "| Sentiment Analysis and Brand Reputation | Enables AI systems to accurately analyze sentiment and identify trends in customer feedback and reviews. | 88% of consumers are more likely to recommend a company to others after a positive customer experience. | Temkin Group. “ROI of Customer Experience, 2018.” |\n",
    "| Productivity and Process Optimization | Enhances internal processes, increasing efficiency and productivity within organizations. | Automation technologies could increase productivity by 0.8% to 1.4% annually, saving almost 15 trillion in wages. | McKinsey Global Institute. “A future that works: Automation, employment, and productivity.” |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering with LLM Model Like ChatGPT\n",
    "\n",
    "Prompt engineering is a key factor in harnessing the full potential of AI models like ChatGPT. A well-crafted prompt can guide the model to generate accurate, relevant, and coherent responses, while minimizing errors and misunderstandings. Conversely, poorly constructed prompts can lead to irrelevant, ambiguous, or even incorrect outputs. Therefore, investing time in crafting efficient prompts is crucial for obtaining the best results from ChatGPT. Using following aspects you can format your prompt, properly.\n",
    "\n",
    "\n",
    "| Aspect | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Clear and Specific Instructions | Provide explicit instructions on the desired task and how the model should approach it. | \"Compose a four-line poem on the subject of love, utilizing the ABAB rhyme scheme.\" |\n",
    "| Examples and Templates | Illustrate the desired output by including examples or templates. | \"Summary: [one sentence capturing the article’s main idea].\" |\n",
    "| Keywords and Cues | Employ keywords and cues to guide the model’s attention toward relevant information and context. | \"Alice: [a young witch eager to learn magic]. Bob: [a wise wizard instructing Alice]. Dialogue:\" |\n",
    "| Feedback and Refinement | Evaluate the model’s output and provide feedback for refinement. | \"Slogan for a new toothpaste that whitens teeth and freshens breath. Output: [suggested slogan]. Feedback: [too long/too boring/too generic, etc.]. Refinement: [suggestion for improvement].\" |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Engineering Techniques\n",
    "\n",
    "We define principles and strategies to consider creating prompts. Before we delve into them, here is the summary table:\n",
    "\n",
    "\n",
    "\n",
    "| Principle | Strategies | Description | Example |\n",
    "|---|---|---|---|\n",
    "| **Principle 1: Write Clear and Specific Instructions** | Strategy 1: Use Delimiters to Clearly Indicate Distinct Parts of the Input | Delimiters help avoid potential interference from misleading user input. | `prompt = \"\"\"Summarize the text delimited by triple backticks into a single sentence.```{text}```\"` |\n",
    "| | Strategy 2: Ask for Structured Output | This approach helps make model outputs directly usable for programs, such as JSON outputs that can be read and converted into dictionary format by Python programs. | `prompt = \"\"\"Generate a list of three made-up book titles along with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre.\"\"\"` |\n",
    "| | Strategy 3: Check Whether Conditions are Satisfied | If the completion of the task has preconditions that must be met, we should require the model to check these conditions first and instruct it to stop trying if they are not met. | `prompt = \"\"\"You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, re-write those instructions in the following format: Step 1 - ... Step 2 - … … Step N - … If the text does not contain a sequence of instructions, then simply write \"No steps provided.\"\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\"\"\"` |\n",
    "| | Strategy 4: “N-shot” Prompting | Providing the model with one or more sample prompts helps clarify the expected output. | `prompt = \"\"\"Your task is to answer in a consistent style.<child>: Teach me about patience.<grandparent>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.<child>: Teach me about resilience.\"\"\"` |\n",
    "| **Principle 2: Give the Model Time to “Think”** | Strategy 1: Specify the Steps Required to Complete a Task | By providing the necessary steps, the model can reference the results of previous steps and improve the accuracy of the output. | `prompt = \"\"\"Your task is to perform the following actions: 1 - Summarize the following text delimited by <> with 1 sentence. 2 - Translate the summary into French. 3 - List each name in the French summary. 4 - Output a json object that contains the following keys: french_summary, num_names.\"\"\"` |\n",
    "| | Strategy 2: Instruct the Model to Work Out Its Own Solution Before Rushing to a Conclusion | If the task is too complicated or the description is too little, then the model can only draw conclusions by guessing. So, in this case, we can instruct the model to take longer to think about the problem. | `prompt = \"\"\"Your task is to determine if the student's solution is correct or not. To solve the problem do the following: - First, work out your own solution to the problem. - Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\"\"\"` |\n",
    "| **Additional Strategies** | Chain-of-Thought (CoT) Prompting | CoT prompting prompts the model to produce intermediate reasoning steps before giving the final answer to a multi-step problem. |  `prompt = \"\"\" Your task is to solve the following math problem: 2x^2 - 3x + 1 = 0. Break down the problem into simpler subproblems, solve each one in sequence, and then combine the solutions to solve the original problem.\"\"\"` |\n",
    "| | Generated Knowledge Prompting | The idea behind the generated knowledge prompting is to ask the AI to generate potentially useful information about a given question/prompt, and then leverage that provided knowledge as additional input for generating a final | `prompt: \"\"\"Before writing an article about cybersecurity, particularly cookie theft, generate some dangers and protections against cookie theft. \"\"\"prompt = f\"\"\" Now, using the generated knowledge, write an article about cybersecurity with a focus on cookie theft. Dangers: {response['dangers']} Protections: {response['protections']} \"\"\"` In this example, the AI is first asked to generate some dangers and protections against cookie theft. Then, it is asked to use this generated knowledge to write an article about cybersecurity.  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle 1: Write Clear and Specific Instructions\n",
    "\n",
    "Ensure your prompts are clear and concise to help the model understand the intent and desired output. Avoid ambiguous language or phrasing that could lead to multiple interpretations. This can be accomplished with strategies such as:\n",
    "\n",
    "#### Strategy 1: Use Delimiters to Clearly Indicate Distinct Parts of the Input\n",
    "\n",
    "Delimiters help avoid potential interference from misleading user input. Examples of delimiters include:\n",
    "\n",
    "- Triple quotes: `\"\"\"`\n",
    "- Triple backticks: ```` ``` ````\n",
    "- Triple dashes: `---`\n",
    "- Angle brackets: `<>`\n",
    "- XML tags: `<tag>`\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "text = \"\"\"\n",
    "You should express what you want a model to do by providing instructions that are as clear and specific as you can possibly make them. This will guide the model towards the desired output, and reduce the chances of receiving irrelevant or incorrect responses. Don't confuse writing a clear prompt with writing a short prompt. In many cases, longer prompts provide more clarity and context for the model, which can lead to more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Strategy 2: Ask for Structured Output\n",
    "\n",
    "This approach helps make model outputs directly usable for programs, such as JSON outputs that can be read and converted into dictionary format by Python programs.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Generate a list of three made-up book titles along with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "<mark> Result:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"book_id\": 1,\n",
    "    \"title\": \"The Lost City of Zorath\",\n",
    "    \"author\": \"Aria Blackwood\",\n",
    "    \"genre\": \"Fantasy\"\n",
    "  },\n",
    "  {\n",
    "    \"book_id\": 2,\n",
    "    \"title\": \"The Last Survivors\",\n",
    "    \"author\": \"Ethan Stone\",\n",
    "    \"genre\": \"Science Fiction\"\n",
    "  },\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "#### Strategy 3: Check Whether Conditions are Satisfied\n",
    "\n",
    "If the completion of the task has preconditions that must be met, we should require the model to check these conditions first and instruct it to stop trying if they are not met.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "text_1 = \"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some water boiling. While that's happening, grab a cup and put a tea bag in it. Once the water is hot enough, just pour it over the tea bag. Let it sit for a bit so the tea can steep. After a few minutes, take out the tea bag. If you like, you can add some sugar or milk to taste. And that's it! You've got yourself a delicious cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, re-write those instructions in the following format:\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "If the text does not contain a sequence of instructions, then simply write \"No steps provided.\"\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "<mark> Output:\n",
    "```\n",
    "Step 1 - Get some water boiling.\n",
    "Step 2 - Grab a cup and put a tea bag in it.\n",
    "Step 3 - Once the water is hot enough, pour it over the tea bag.\n",
    "Step 4 - Let it sit for a bit so the tea can steep.\n",
    "Step 5 - After a few minutes, take out the tea bag.\n",
    "Step 6 - If you like, you can add some sugar or milk to taste.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Strategy 4: “N-shot” Prompting\n",
    "\n",
    "Providing the model with one or more sample prompts helps clarify the expected output.\n",
    "Providing the model with one or more sample prompts helps clarify the expected output.\n",
    "N-shot prompting, including Zero-shot and Few-shot prompting, refers to the number of \"training\" examples or clues given to the model to make predictions.\n",
    "\n",
    "| Type | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Zero-shot prompting | The model makes predictions without any additional training. This works for common straightforward problems like classification or text transformation. | classification (i.e. sentiment analysis, spam classification => \"Is this email spam or not? ==> <i>PASTE THE EMAIL CONTENT</i> \"), text transformation (i.e. translation, summarizing, expanding), and simple text generation on which the LLM has been largely trained  |\n",
    "| Few-shot prompting | Uses a small amount of data (typically between two and five) to adapt its output based on these small examples. These examples are meant to steer the model to better performance for a more context-specific problem. | \"Translate the following English sentences to French: 'Hello, how are you?' 'I am fine, thank you.'\" |\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "<child>: Teach me about patience.\n",
    "<grandparent>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "<mark> Output:\n",
    "```\n",
    "<grandparent>: Resilience is like a tree that bends but does not break in the face of a storm. It's about facing life's challenges and bouncing back stronger than before.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Principle 2: Give the Model Time to “Think”\n",
    "\n",
    "This principle utilizes the idea of a thought chain, breaking complex tasks into N sequential subtasks, allowing the model to think step-by-step and produce more accurate outputs.\n",
    "\n",
    "#### Strategy 1: Specify the Steps Required to Complete a Task\n",
    "\n",
    "Here’s an example involving summarizing text, translating it into French, listing names in the French summary, and finally outputting data in JSON format. By providing the necessary steps, the model can reference the results of previous steps and improve the accuracy of the output.\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in French summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "<mark> Outcome example:\n",
    "```\n",
    "Text: <John and Mary went to the park. They played football and had a picnic.>\n",
    "Summary: <John and Mary enjoyed a day at the park playing football and having a picnic.>\n",
    "Translation: <John et Mary ont passé une journée agréable au parc en jouant au football et en pique-niquant.>\n",
    "Names: <['John', 'Mary']>\n",
    "Output JSON: <{\"french_summary\": \"John et Mary ont passé une journée agréable au parc en jouant au football et en pique-niquant.\", \"num_names\": 2}>\n",
    "\n",
    "```\n",
    "\n",
    "#### Strategy 2: Instruct the Model to Work Out Its Own Solution Before Rushing to a Conclusion\n",
    "\n",
    "If the task is too complicated or the description is too little, then the model can only draw conclusions by guessing. So, in this case, we can instruct the model to take longer to think about the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Strategies\n",
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Introduced by Google researchers in 2022, CoT prompting prompts the model to produce intermediate reasoning steps before giving the final answer to a multi-step problem. This method enables models to decompose multi-step problems into intermediate steps, enabling them to solve complex reasoning problems that are not solvable with standard prompting methods.\n",
    "\n",
    "Absolutely, here are examples for each type of Chain-of-Thought (CoT) prompting:\n",
    "\n",
    "| Type | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Self-consistency prompting | Sample multiple diverse reasoning paths and select the most consistent answers. | If the task is to predict the weather, the model might generate multiple predictions like \"It will rain\", \"It will be cloudy\", and \"It will be sunny\". The model then selects the most consistent answer. |\n",
    "| Least-to-Most prompting (LtM) | Specify the chain of thought to first break a problem into a series of simpler subproblems and then solve them in sequence. | If the task is to solve a complex math problem, the model first breaks it down into simpler subproblems, solves each one in sequence, and then combines the solutions to solve the original problem. |\n",
    "| Active Prompting | Determine which questions are the most important and helpful ones for human annotation. | If the task is to write a detailed report, the model first identifies the most uncertain areas (e.g., specific data points or arguments), asks for human annotation on these areas, and then incorporates the annotated information into the final report. |\n",
    "\n",
    "### Generated Knowledge Prompting\n",
    "\n",
    "The idea behind the generated knowledge prompting is to ask the AI to generate potentially useful information about a given question/prompt, and then leverage that provided knowledge as additional input for generating a final response.\n",
    "\n",
    "For example, say you want to write an article about cybersecurity, particularly cookie theft. Before asking the AI to write the article, you can ask it to generate some dangers and protections against cookie theft. This will help the AI write a more informative blog post.\n",
    "\n",
    "In addition to these techniques, a new method called \"Prompt Tuning\" has been introduced, which involves learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. This approach outperforms GPT-3's \"few-shot\" learning by a large margin and becomes more competitive with scale. As models exceed billions of parameters, this method matches the strong performance of model tuning (where all model weights are tuned) [source](https://arxiv.org/abs/2104.08691).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Into Practice With Python Project\n",
    "\n",
    "Let's imagine you're interested in learning survival analysis in Python, but you have limited knowledge about this technique and its practical applications. We can leverage the power of ChatGPT and effective prompt engineering to explore and understand survival analysis together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's embark on this learning journey together. We'll use the principles of prompt engineering to guide our exploration of survival analysis in Python. Here's how we might structure our inquiry:\n",
    "\n",
    "1. **Understanding the Concept**: First, we need to understand what survival analysis is. We can ask ChatGPT to provide a definition and explanation of survival analysis.\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Please provide a detailed explanation of survival analysis, including its definition, purpose, and applications.\n",
    "\"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to see the outcome from chatGPT</summary>\n",
    "\n",
    "## Person: \"Please provide a detailed explanation of survival analysis, including its definition, purpose, and applications.\"\n",
    "## ChatGPT:\n",
    "\n",
    "Survival analysis is a branch of statistics that deals with the analysis of time-to-event data. These are situations where the outcome of interest is not just whether an event happened, but when it happened. The \"event\" in question is often death (hence the term \"survival analysis\"), but it can be any event of interest: the failure of a machine, the recovery of a patient, the relapse of a disease, the churn of a customer, etc.\n",
    "\n",
    "Here are some key points about survival analysis:\n",
    "\n",
    "**Definition and Purpose**\n",
    "\n",
    "Survival analysis is used to analyze the time until the occurrence of an event of interest. The goal is to estimate and interpret survivor functions and hazard functions. \n",
    "\n",
    "- The survivor function, denoted S(t), is the probability that the event of interest has not yet occurred by time t.\n",
    "- The hazard function, denoted h(t), is the event rate at time t conditional on survival until time t (i.e., the risk of the event per time unit).\n",
    "\n",
    "Survival analysis takes into account censoring, which occurs when we have some information about individual survival time, but we don’t know the survival time exactly (for example, if a patient is still alive at the end of the study, or if a customer has not yet churned by the time data were collected).\n",
    "\n",
    "**Applications**\n",
    "\n",
    "Survival analysis has applications in many fields:\n",
    "\n",
    "1. **Medicine**: This is the field where survival analysis is most used. It's often used to measure the fraction of patients living for a certain amount of time after treatment. In this context, the event of interest is often death, but can also be the occurrence of a disease, or recovery from surgery.\n",
    "\n",
    "2. **Biology**: Survival analysis can be used to measure lifetimes of species.\n",
    "\n",
    "3. **Engineering**: In reliability engineering, it can be used to analyze time-to-failure data.\n",
    "\n",
    "4. **Marketing and Business**: Survival analysis can be used to predict customer churn and understand customer retention.\n",
    "\n",
    "5. **Social Sciences**: It can be used to analyze the time until events like marriage, divorce, or retirement.\n",
    "\n",
    "**Methods**\n",
    "\n",
    "There are several methods used in survival analysis, including the Kaplan-Meier estimator, the Nelson-Aalen estimator, and Cox proportional hazards models. These methods have different strengths and assumptions, and the choice of method depends on the specifics of the problem at hand.\n",
    "\n",
    "- The Kaplan-Meier estimator is a non-parametric statistic used to estimate the survival function from lifetime data. It allows for varying survival times and can handle censored data.\n",
    "\n",
    "- The Nelson-Aalen estimator is another non-parametric statistic used to estimate the cumulative hazard function in survival analysis.\n",
    "\n",
    "- Cox proportional hazards model (or Cox regression model) is a semi-parametric model that allows the inclusion of multiple covariates and assumes that the effects of the covariates are multiplicative with respect to the hazard rate and are constant over time.\n",
    "\n",
    "In conclusion, survival analysis is a powerful tool for analyzing time-to-event data, with applications in many fields. It's particularly useful when dealing with censored data, which are common in many research areas.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Learning the Basics**: Next, we can ask ChatGPT to explain the basic principles and techniques used in survival analysis.\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Could you explain the basic principles and techniques used in survival analysis?\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to see the outcome from chatGPT</summary>\n",
    "\n",
    "## Person: \"Could you explain the basic principles and techniques used in survival analysis?\"\n",
    "## ChatGPT:\n",
    "\n",
    "Sure, I'd be happy to explain the basic principles and techniques used in survival analysis. \n",
    "\n",
    "**Basic Principles**\n",
    "\n",
    "1. **Survival Function (S(t))**: This is the probability that a subject survives from the time origin (t=0) to a specified future time t. Survival function is defined as S(t) = Pr(T > t), where T is the random variable denoting the time of death.\n",
    "\n",
    "2. **Hazard Function (h(t))**: This is the event rate at time t conditional on survival until time t or later (i.e., T ≥ t). It can be interpreted as the risk of dying at time t. It is formally defined as the derivative of the survival function.\n",
    "\n",
    "3. **Censoring**: This is a type of missing data problem common in survival analysis. Right censoring is the most common type, which happens if a patient is still alive (or the event has not occurred) at the end of the observation period. The exact survival time for this patient is not known, but it is known to be longer than a certain time.\n",
    "\n",
    "**Techniques**\n",
    "\n",
    "1. **Kaplan-Meier Estimator**: This is a non-parametric statistic used to estimate the survival function from lifetime data. In medical research, it is often used to measure the fraction of patients living for a certain amount of time after treatment. The estimator is named after Edward L. Kaplan and Paul Meier, who each submitted similar manuscripts to the Journal of the American Statistical Association. The journal editor, John Tukey, convinced them to combine their work into one paper, which has since been cited over 50,000 times.\n",
    "\n",
    "2. **Log-Rank Test**: This is a hypothesis test to compare the survival distributions of two samples. It is used to test the null hypothesis that the survival distributions are the same across the groups. The test statistic is a sum of the observed and expected number of events in one of the groups at each observed event time.\n",
    "\n",
    "3. **Cox Proportional Hazards Model**: This is a semi-parametric model developed by Sir David Cox in 1972. It allows us to estimate the hazard (or risk) of death, based on the values of covariates for each individual. The model's name is derived from the proportional hazards assumption, which implies that the hazard functions of individuals are proportional and the ratio of hazard functions is constant over time.\n",
    "\n",
    "4. **Accelerated Failure Time Model (AFT)**: This is a parametric model that provides an alternative to the Cox model. Instead of modelling the hazard function, the AFT model focuses on the survival time. The model assumes that covariates have a multiplicative effect on the survival time.\n",
    "\n",
    "5. **Parametric Survival Models**: These are models that assume a particular distribution for the survival times (e.g., exponential, Weibull, Gompertz). They can provide a better fit to data than the Cox model if the distributional assumption is correct, and they can provide more interpretable summaries of effects.\n",
    "\n",
    "These are just a few of the basic principles and techniques used in survival analysis. The choice of technique depends on the specifics of the problem at hand, including the nature of the data and the research question.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Python Implementation**: Once we understand the basics, we can ask ChatGPT to guide us on how to implement survival analysis in Python. This could include asking for recommended libraries, sample code, and resources for further learning.\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "What are the recommended Python libraries for performing survival analysis? Could you provide a simple example of how to implement survival analysis using Python?\n",
    "\"\"\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <mark> ChatGPT response:\n",
    "\n",
    "\n",
    "> Sure, I can provide a simple example of how to perform survival analysis using Python. The most commonly used Python libraries for survival analysis are `lifelines` and `scikit-survival`.\n",
    ">\n",
    "> Here is a simple example of how to perform survival analysis using the `lifelines` library. We will use the Kaplan-Meier estimator, which is a non-parametric method used to estimate the survival function from lifetime data.\n",
    ">\n",
    "> First, we need to install the necessary library. Let's install `lifelines`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lifelines==0.27.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, let's import the necessary libraries and load some data. We will use the built-in dataset from `lifelines` called `lifelines.datasets.load_dd`. This dataset is a sample of individuals who had undergone a certain type of liver disease surgery. The dataset contains the following columns:\n",
    ">\n",
    "> - ctryname: Country name\n",
    "> - cowcode2: Country code\n",
    "> - politycode: Polity code\n",
    "> - un_region_name: United Nations region name\n",
    "> - un_continent_name: United Nations continent name\n",
    "> - ehead: Executive head\n",
    "> - leaderspellreg: Leader's spell regime\n",
    "> - democracy: Democracy status\n",
    "> - regime: Regime type\n",
    "> - start_year: Start year of the regime\n",
    "> - duration: Duration of the regime\n",
    "> - observed: Observation status\n",
    ">\n",
    ">\n",
    "> Let's load the data and take a look at the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lifelines.datasets import load_dd\n",
    "\n",
    "data = load_dd()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now that we have loaded the data, let's perform survival analysis using the Kaplan-Meier estimator. This estimator is a non-parametric statistic used to estimate the survival function from lifetime data.\n",
    ">\n",
    "> We will create an instance of the `KaplanMeierFitter` class and then fit it to our data using the `fit` method. The `fit` method takes two arguments: the durations (length of the political regime) and the event observed (1 if the end of the regime was observed, 0 otherwise).\n",
    ">\n",
    "> After fitting the model, we can plot the survival function using the `plot_survival_function` method. This will provide us with a visual representation of the estimated survival function for political regimes, where the y-axis represents the probability of a regime surviving after a given amount of time (x-axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lifelines.datasets import load_dd\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dd()\n",
    "\n",
    "# Create an instance of KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Fit the data into the model\n",
    "kmf.fit(durations = data['duration'], event_observed = data['observed'])\n",
    "\n",
    "# Create an estimate of the survival function\n",
    "kmf.plot_survival_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \"N-shot\" Prompting strategy to Ask Further Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Prompt= \"\"\"\n",
    "Calculate and interpret the median survival time from the Kaplan-Meier survival analysis. Explain what the median survival time represents and how it is calculated. Provide Python code for this analysis\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark> ChatGPT Response:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The median survival time is the time at which the survival probability drops to 50%. In other words, it is the time at which the estimated survival function S(t) equals 0.5. It represents the time by which half of the population has experienced the event of interest.\n",
    ">\n",
    "> The median survival time is a commonly used summary statistic in survival analysis because it is easy to interpret and does not depend on the entire survival distribution. It provides a measure of the central tendency of the survival times.\n",
    ">\n",
    "> In the Kaplan-Meier survival analysis, the median survival time can be calculated using the `median_survival_time_` attribute of the fitted `KaplanMeierFitter` object.\n",
    ">\n",
    "> Here is how you can calculate the median survival time in Python using the `lifelines` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Create an instance of KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# Fit the data into the model\n",
    "kmf.fit(durations = data['duration'], event_observed = data['observed'])\n",
    "\n",
    "# Calculate the median survival time\n",
    "median_survival_time = kmf.median_survival_time_\n",
    "\n",
    "print(\"The median survival time is:\", median_survival_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This code will print the median survival time of the political regimes in the dataset. Please note that you need to have the `lifelines` library installed in your Python environment to run this code. You can install it using the command `!pip install lifelines`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering with Langchain\n",
    "\n",
    "LangChain is a powerful library that leverages the potential of prompts for working with LLMs. It offers a set of objects called PromptTemplates that let you customize and optimize your prompts for different tasks and scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.18\n",
    "!pip install pydantic==1.10.9\n",
    "#!pip install OpenAI\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have installed that, please **restart your kernel**. You can do that by navigating to the button shown below:\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/build-a-hotdog-not-hotdog-classifier-guided-project/images/Restarting_the_Kernel.png\" width=\"60%\" alt=\"Restart kernel\"> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to insert your LLM API key. As an example, here's a demonstration to show you how to get your hugging face API key (which is free).\n",
    "\n",
    "Initialize HuggingFace API key from your account by following steps: \n",
    "\n",
    "1. Go to the https://huggingface.co/\n",
    "2. Log in to your account (or sign up free if it is your first time)\n",
    "3. Go to Settings -> Access Tokens -> click on New Token (image blew)\n",
    "4. Select either read or write option and copy the token\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0DTPEN/images/HF%20key%201.png\n",
    "\" width=\"600\" alt=\"\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0DTPEN/images/HF%20key%202.png\n",
    "\" width=\"600\" alt=\"\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To read a book effectively, you should start by setting aside a specific time and place for reading. This will help you stay focused and avoid distractions. You should also choose a book that interests you and that you are excited to read. Additionally, try to read in a comfortable position and take breaks as needed. Finally, try to engage with the material by asking questions and making connections to what you already know.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# set up the environment with respected API key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# you can choose between different llm models\n",
    "\n",
    "# The \"temperature\" is a hyperparameter that controls the randomness of the model's output. A lower value (like 0.1) makes the output more deterministic, while a higher value makes it more random.\n",
    "# \"max_new_tokens\" parameter sets a limit on the maximum number of new tokens (words/characters) that the model can generate as output.\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\",model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 600})\n",
    "\n",
    "\n",
    "# you can use OpenAI GPT models\n",
    "#llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "text = \"How read book effectively?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to see the sample of openAI outcome</summary>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0TGVEN/images/langchain_sample_notebook.png\" alt=\"output\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template in Langchain\n",
    "\n",
    "A prompt template in LangChain is a class that allows you to create and format prompts for LLMs with dynamic inputs. A prompt template has input variables, a template string, and an optional output parser. You can use different types of prompt templates for different tasks and scenarios, such as few-shot learning, chatbots, question answering, etc. For more information visit 🦜🔗 [LangChain.](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html)\n",
    "\n",
    "\n",
    "For example, here is a prompt template for step by step instruction a text and present it in table-like format. This prompt can then be passed to an LLM to generate the desired output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 020a7f40-bcf6-4a93-919d-11f87d51e68c)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m             )\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m             )\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 788\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             )\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m             )\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_423/1510335549.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Print the formatted prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/langchain/llms/huggingface_hub.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[1;32m     94\u001b[0m         \u001b[0m_model_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by inference API: {response['error']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/huggingface_hub/inference_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, params, data, raw_response)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Make API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Let the user handle the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \"\"\"\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;34m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 020a7f40-bcf6-4a93-919d-11f87d51e68c)')"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "# Define the template\n",
    "template = \"\"\"\n",
    "Give me step by step instruction in table format:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template object\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"], # The name of the input variable\n",
    "    template=template # The template string\n",
    ")\n",
    "\n",
    "# Format the prompt with some text\n",
    "text = \"I want to backflip\"\n",
    "formatted_prompt = summary_prompt.format(text=text)\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(llm(formatted_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to see the outcome (the output is from the OpenAI llm)</summary>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0TGVEN/images/langchain_notebook_prompt_output.png\" alt=\"output\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering With IBM Watsonx\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0TGVEN/images/watsonX.jpeg\" width=\"60%\" alt=\"Restart kernel\"> </center>\n",
    "\n",
    "IBM has a special offer for watsonx.ai, a studio for new foundation models, generative AI and machine learning. To take advantage of this offer visit [watsonx.ai homepage](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMSkillsNetwork-GPXX0TGVEN&context=wx&apps=data_science_experience%2Cwatson_data_platform%2Ccos). \n",
    "\n",
    "\n",
    "In the Prompt Lab in [IBM watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMSkillsNetwork-GPXX0TGVEN&context=wx&apps=data_science_experience%2Cwatson_data_platform%2Ccos), you can experiment with prompting different foundation models, explore sample prompts, as well as save and share your best prompts. \n",
    "\n",
    "\n",
    "<iframe width=\"480\" height=\"270\" src=\"https://video.ibm.com/embed/recorded/132848516\" scrolling=\"no\" allowfullscreen webkitallowfullscreen frameborder=\"0\" style=\"border: 0 none transparent;\"></iframe>\n",
    "\n",
    "\n",
    "The IBM watsonx prompt lab is a graphical interface that allows you to experiment with prompting different foundation models, explore sample prompts, as well as save and share your best prompts¹. The prompt lab helps you craft effective prompts by providing different modes, parameters, and feedback. You can also learn from documented samples and tips for writing foundation model prompts². The prompt lab is part of the IBM watsonx platform, which is a cloud-based service that enables you to access, analyze, and build with LLM models³.\n",
    "\n",
    "\n",
    "\n",
    "1. Prompt Lab - IBM. https://www.ibm.com/docs/en/watsonx?topic=models-prompt-lab.\n",
    "2. Tips for writing foundation model prompts: prompt engineering - IBM. https://www.ibm.com/docs/en/watsonx?topic=models-prompt-tips.\n",
    "3. Prompt Lab | IBM watsonx. https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-prompt-lab.html?context=wx.\n",
    "\n",
    "\n",
    "Here is the example of how WatsonX platform facilitate prompt engineering: \n",
    "\n",
    "> **Before:**\n",
    "\n",
    "> In this image, you can see a prompt with the original, simple instruction. This prompt doesn't produce great results.\n",
    "\n",
    "<center> <img src=\"https://www.ibm.com/docs/en/SSYOK8/wsj/analyze-data/images/fm-prompt-lab-prompt-too-simple.png\" width=\"80%\" alt=\"watsonX\"> </center>\n",
    "\n",
    "\n",
    "> Example prompt text with just a simple instruction\n",
    "\n",
    "> **After:**\n",
    "<center> <img src=\"https://www.ibm.com/docs/en/SSYOK8/wsj/analyze-data/images/fm-prompt-lab-prompt-components.png\" width=\"80%\" alt=\"watsonX\"> </center>\n",
    "\n",
    "> In this image, you can see all the prompt components: instruction (complete with descriptive details), context, example, and cue. This prompt produces a much better result.\n",
    "\n",
    "\n",
    "You can experiment with this prompt in the Prompt Lab yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Writing efficient ChatGPT prompts is critical to getting high-quality, relevant, and contextually appropriate responses. By applying the techniques discussed in this article, you can optimize the use of ChatGPT in various applications, such as chatbots, language translation, and content generation. Mastering the art of prompt engineering can truly unlock the power of AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| - | - |\n",
    "|---|---|\n",
    "| **Sina Nazeri (PhD.)**  | [linkedin](https://www.linkedin.com/in/sina-nazeri) |\n",
    "| ![Sina](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0Q8REN/images/sina_low_qual.jpeg) | As a data scientist in IBM, I have always been passionate about sharing my knowledge and helping others learn about the field. I believe that everyone should have the opportunity to learn about data science, regardless of their background or experience level. This belief has inspired me to become a learning content provider, creating and sharing educational materials that are accessible and engaging for everyone. |\n",
    "Sina Nazeri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOSEPH SANTARCANGELO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-06-17|0.1|Sina Nazeri|Create Lab |\n",
    "|2023-09-07|0.2|Sina Nazeri|changed openAI api to huggingface api|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
